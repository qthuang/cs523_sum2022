{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejjaG1287O7t","executionInfo":{"status":"ok","timestamp":1660187810526,"user_tz":240,"elapsed":14666,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}},"outputId":"a46507e8-ff0f-4d7b-84c6-79b329c16733"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scipy==1.2.1\n","  Downloading scipy-1.2.1-cp37-cp37m-manylinux1_x86_64.whl (24.8 MB)\n","\u001b[K     |████████████████████████████████| 24.8 MB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from scipy==1.2.1) (1.21.6)\n","Installing collected packages: scipy\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.2.1 which is incompatible.\n","jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.2.1 which is incompatible.\n","jax 0.3.14 requires scipy>=1.5, but you have scipy 1.2.1 which is incompatible.\u001b[0m\n","Successfully installed scipy-1.2.1\n"]}],"source":["!pip install scipy==1.2.1"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import sys\n","sys.path.insert(0,'/content/drive/My Drive/CS523/Project/face_classification/src')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHhpkZ9C-GXf","executionInfo":{"status":"ok","timestamp":1660187827578,"user_tz":240,"elapsed":17061,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}},"outputId":"ee88dbc9-bd16-4ee7-b26b-99ad103cddc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/My Drive/CS523/Project/face_classification/src"],"metadata":{"id":"XXNIvtwAVkMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n","from keras.callbacks import *\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.preprocessing.image import ImageDataGenerator"],"metadata":{"id":"wdW9ozyJ8Cvk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from models.cnn import mini_XCEPTION\n","from utils.datasets import DataManager\n","from utils.datasets import split_data\n","from utils.preprocessor import preprocess_input\n","\n","import tensorflow as tf\n","from matplotlib import pyplot as plt"],"metadata":{"id":"7GMKncqh78J9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mz0uS8e07O7x","executionInfo":{"status":"ok","timestamp":1660187904799,"user_tz":240,"elapsed":3,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}},"outputId":"baaa2158-6d29-44d1-de12-0959cbf8be77"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}],"source":["print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nm_AMlzm7O7y","executionInfo":{"status":"ok","timestamp":1660187905788,"user_tz":240,"elapsed":140,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}},"outputId":"bef26946-580b-4c28-feda-4bb25a3ee697"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  1\n"]}],"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cse8TPp7O7z"},"outputs":[],"source":["# parameters\n","batch_size = 32\n","num_epochs = 10000\n","input_shape = (64, 64, 1)\n","validation_split = .2\n","verbose = 1\n","num_classes = 7\n","patience = 50\n","base_path = '../trained_models/emotion_models/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yt5EAujY7O70"},"outputs":[],"source":["# data generator\n","data_generator = ImageDataGenerator(\n","                        featurewise_center=False,\n","                        featurewise_std_normalization=False,\n","                        rotation_range=10,\n","                        width_shift_range=0.1,\n","                        height_shift_range=0.1,\n","                        zoom_range=.1,\n","                        horizontal_flip=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IArMe91w7O71","executionInfo":{"status":"ok","timestamp":1660187964576,"user_tz":240,"elapsed":3812,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}},"outputId":"475a022a-1c67-4643-c5b1-345bc99fdebc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 64, 64, 1)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 62, 62, 8)    72          ['input_1[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 62, 62, 8)   32          ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," activation (Activation)        (None, 62, 62, 8)    0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 60, 60, 8)    576         ['activation[0][0]']             \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 60, 60, 8)   32          ['conv2d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_1 (Activation)      (None, 60, 60, 8)    0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," separable_conv2d (SeparableCon  (None, 60, 60, 16)  200         ['activation_1[0][0]']           \n"," v2D)                                                                                             \n","                                                                                                  \n"," batch_normalization_3 (BatchNo  (None, 60, 60, 16)  64          ['separable_conv2d[0][0]']       \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_2 (Activation)      (None, 60, 60, 16)   0           ['batch_normalization_3[0][0]']  \n","                                                                                                  \n"," separable_conv2d_1 (SeparableC  (None, 60, 60, 16)  400         ['activation_2[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_4 (BatchNo  (None, 60, 60, 16)  64          ['separable_conv2d_1[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv2d_2 (Conv2D)              (None, 30, 30, 16)   128         ['activation_1[0][0]']           \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 30, 30, 16)   0           ['batch_normalization_4[0][0]']  \n","                                                                                                  \n"," batch_normalization_2 (BatchNo  (None, 30, 30, 16)  64          ['conv2d_2[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add (Add)                      (None, 30, 30, 16)   0           ['max_pooling2d[0][0]',          \n","                                                                  'batch_normalization_2[0][0]']  \n","                                                                                                  \n"," separable_conv2d_2 (SeparableC  (None, 30, 30, 32)  656         ['add[0][0]']                    \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_6 (BatchNo  (None, 30, 30, 32)  128         ['separable_conv2d_2[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_3 (Activation)      (None, 30, 30, 32)   0           ['batch_normalization_6[0][0]']  \n","                                                                                                  \n"," separable_conv2d_3 (SeparableC  (None, 30, 30, 32)  1312        ['activation_3[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_7 (BatchNo  (None, 30, 30, 32)  128         ['separable_conv2d_3[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," conv2d_3 (Conv2D)              (None, 15, 15, 32)   512         ['add[0][0]']                    \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 32)  0           ['batch_normalization_7[0][0]']  \n","                                                                                                  \n"," batch_normalization_5 (BatchNo  (None, 15, 15, 32)  128         ['conv2d_3[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_1 (Add)                    (None, 15, 15, 32)   0           ['max_pooling2d_1[0][0]',        \n","                                                                  'batch_normalization_5[0][0]']  \n","                                                                                                  \n"," separable_conv2d_4 (SeparableC  (None, 15, 15, 64)  2336        ['add_1[0][0]']                  \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_9 (BatchNo  (None, 15, 15, 64)  256         ['separable_conv2d_4[0][0]']     \n"," rmalization)                                                                                     \n","                                                                                                  \n"," activation_4 (Activation)      (None, 15, 15, 64)   0           ['batch_normalization_9[0][0]']  \n","                                                                                                  \n"," separable_conv2d_5 (SeparableC  (None, 15, 15, 64)  4672        ['activation_4[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_10 (BatchN  (None, 15, 15, 64)  256         ['separable_conv2d_5[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_4 (Conv2D)              (None, 8, 8, 64)     2048        ['add_1[0][0]']                  \n","                                                                                                  \n"," max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)    0           ['batch_normalization_10[0][0]'] \n","                                                                                                  \n"," batch_normalization_8 (BatchNo  (None, 8, 8, 64)    256         ['conv2d_4[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," add_2 (Add)                    (None, 8, 8, 64)     0           ['max_pooling2d_2[0][0]',        \n","                                                                  'batch_normalization_8[0][0]']  \n","                                                                                                  \n"," separable_conv2d_6 (SeparableC  (None, 8, 8, 128)   8768        ['add_2[0][0]']                  \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_12 (BatchN  (None, 8, 8, 128)   512         ['separable_conv2d_6[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_5 (Activation)      (None, 8, 8, 128)    0           ['batch_normalization_12[0][0]'] \n","                                                                                                  \n"," separable_conv2d_7 (SeparableC  (None, 8, 8, 128)   17536       ['activation_5[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_13 (BatchN  (None, 8, 8, 128)   512         ['separable_conv2d_7[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_5 (Conv2D)              (None, 4, 4, 128)    8192        ['add_2[0][0]']                  \n","                                                                                                  \n"," max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 128)   0           ['batch_normalization_13[0][0]'] \n","                                                                                                  \n"," batch_normalization_11 (BatchN  (None, 4, 4, 128)   512         ['conv2d_5[0][0]']               \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_3 (Add)                    (None, 4, 4, 128)    0           ['max_pooling2d_3[0][0]',        \n","                                                                  'batch_normalization_11[0][0]'] \n","                                                                                                  \n"," separable_conv2d_8 (SeparableC  (None, 4, 4, 256)   33920       ['add_3[0][0]']                  \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_15 (BatchN  (None, 4, 4, 256)   1024        ['separable_conv2d_8[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_6 (Activation)      (None, 4, 4, 256)    0           ['batch_normalization_15[0][0]'] \n","                                                                                                  \n"," separable_conv2d_9 (SeparableC  (None, 4, 4, 256)   67840       ['activation_6[0][0]']           \n"," onv2D)                                                                                           \n","                                                                                                  \n"," batch_normalization_16 (BatchN  (None, 4, 4, 256)   1024        ['separable_conv2d_9[0][0]']     \n"," ormalization)                                                                                    \n","                                                                                                  \n"," conv2d_6 (Conv2D)              (None, 2, 2, 256)    32768       ['add_3[0][0]']                  \n","                                                                                                  \n"," max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 256)   0           ['batch_normalization_16[0][0]'] \n","                                                                                                  \n"," batch_normalization_14 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_6[0][0]']               \n"," ormalization)                                                                                    \n","                                                                                                  \n"," add_4 (Add)                    (None, 2, 2, 256)    0           ['max_pooling2d_4[0][0]',        \n","                                                                  'batch_normalization_14[0][0]'] \n","                                                                                                  \n"," conv2d_7 (Conv2D)              (None, 2, 2, 7)      16135       ['add_4[0][0]']                  \n","                                                                                                  \n"," global_average_pooling2d (Glob  (None, 7)           0           ['conv2d_7[0][0]']               \n"," alAveragePooling2D)                                                                              \n","                                                                                                  \n"," predictions (Activation)       (None, 7)            0           ['global_average_pooling2d[0][0]'\n","                                                                 ]                                \n","                                                                                                  \n","==================================================================================================\n","Total params: 204,087\n","Trainable params: 201,079\n","Non-trainable params: 3,008\n","__________________________________________________________________________________________________\n"]}],"source":["# model parameters/compilation\n","model = mini_XCEPTION(input_shape, num_classes)\n","model.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F0XLL-To7O71","outputId":"64f5e364-c98b-48c9-ca30-d5da37cac029","executionInfo":{"status":"ok","timestamp":1660166534959,"user_tz":240,"elapsed":2809974,"user":{"displayName":"Qintian Huang","userId":"15886403184418412202"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training dataset: fer2013\n","Epoch 1/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.2092 - accuracy: 0.5467\n","Epoch 1: val_loss improved from inf to 1.17472, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.01-0.55.hdf5\n","897/897 [==============================] - 29s 33ms/step - loss: 1.2092 - accuracy: 0.5467 - val_loss: 1.1747 - val_accuracy: 0.5545 - lr: 0.0010\n","Epoch 2/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.1864 - accuracy: 0.5525\n","Epoch 2: val_loss improved from 1.17472 to 1.16601, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.02-0.56.hdf5\n","897/897 [==============================] - 31s 34ms/step - loss: 1.1864 - accuracy: 0.5525 - val_loss: 1.1660 - val_accuracy: 0.5577 - lr: 0.0010\n","Epoch 3/10000\n","897/897 [============================>.] - ETA: 0s - loss: 1.1589 - accuracy: 0.5636\n","Epoch 3: val_loss did not improve from 1.16601\n","897/897 [==============================] - 29s 33ms/step - loss: 1.1590 - accuracy: 0.5636 - val_loss: 1.2315 - val_accuracy: 0.5397 - lr: 0.0010\n","Epoch 4/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.5745\n","Epoch 4: val_loss did not improve from 1.16601\n","897/897 [==============================] - 29s 32ms/step - loss: 1.1354 - accuracy: 0.5745 - val_loss: 1.2488 - val_accuracy: 0.5429 - lr: 0.0010\n","Epoch 5/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.1154 - accuracy: 0.5817\n","Epoch 5: val_loss improved from 1.16601 to 1.12012, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.05-0.58.hdf5\n","897/897 [==============================] - 29s 33ms/step - loss: 1.1154 - accuracy: 0.5817 - val_loss: 1.1201 - val_accuracy: 0.5798 - lr: 0.0010\n","Epoch 6/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.0971 - accuracy: 0.5886\n","Epoch 6: val_loss did not improve from 1.12012\n","897/897 [==============================] - 30s 34ms/step - loss: 1.0971 - accuracy: 0.5886 - val_loss: 1.1430 - val_accuracy: 0.5787 - lr: 0.0010\n","Epoch 7/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.0834 - accuracy: 0.5920\n","Epoch 7: val_loss improved from 1.12012 to 1.11055, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.07-0.59.hdf5\n","897/897 [==============================] - 30s 33ms/step - loss: 1.0834 - accuracy: 0.5920 - val_loss: 1.1105 - val_accuracy: 0.5940 - lr: 0.0010\n","Epoch 8/10000\n","897/897 [============================>.] - ETA: 0s - loss: 1.0627 - accuracy: 0.6008\n","Epoch 8: val_loss improved from 1.11055 to 1.08835, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.08-0.60.hdf5\n","897/897 [==============================] - 29s 32ms/step - loss: 1.0626 - accuracy: 0.6008 - val_loss: 1.0883 - val_accuracy: 0.5975 - lr: 0.0010\n","Epoch 9/10000\n","897/897 [============================>.] - ETA: 0s - loss: 1.0509 - accuracy: 0.6100\n","Epoch 9: val_loss did not improve from 1.08835\n","897/897 [==============================] - 29s 32ms/step - loss: 1.0505 - accuracy: 0.6101 - val_loss: 1.1041 - val_accuracy: 0.5876 - lr: 0.0010\n","Epoch 10/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.0436 - accuracy: 0.6093\n","Epoch 10: val_loss did not improve from 1.08835\n","897/897 [==============================] - 30s 33ms/step - loss: 1.0436 - accuracy: 0.6093 - val_loss: 1.0990 - val_accuracy: 0.6017 - lr: 0.0010\n","Epoch 11/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.0291 - accuracy: 0.6167\n","Epoch 11: val_loss did not improve from 1.08835\n","897/897 [==============================] - 29s 32ms/step - loss: 1.0291 - accuracy: 0.6167 - val_loss: 1.0962 - val_accuracy: 0.5936 - lr: 0.0010\n","Epoch 12/10000\n","898/897 [==============================] - ETA: 0s - loss: 1.0215 - accuracy: 0.6147\n","Epoch 12: val_loss improved from 1.08835 to 1.06403, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.12-0.60.hdf5\n","897/897 [==============================] - 30s 33ms/step - loss: 1.0215 - accuracy: 0.6147 - val_loss: 1.0640 - val_accuracy: 0.5988 - lr: 0.0010\n","Epoch 13/10000\n","897/897 [============================>.] - ETA: 0s - loss: 1.0123 - accuracy: 0.6223\n","Epoch 13: val_loss improved from 1.06403 to 1.03338, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.13-0.63.hdf5\n","897/897 [==============================] - 29s 33ms/step - loss: 1.0123 - accuracy: 0.6222 - val_loss: 1.0334 - val_accuracy: 0.6283 - lr: 0.0010\n","Epoch 14/10000\n","897/897 [============================>.] - ETA: 0s - loss: 1.0009 - accuracy: 0.6249\n","Epoch 14: val_loss did not improve from 1.03338\n","897/897 [==============================] - 29s 33ms/step - loss: 1.0008 - accuracy: 0.6250 - val_loss: 1.0405 - val_accuracy: 0.6137 - lr: 0.0010\n","Epoch 15/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9910 - accuracy: 0.6314\n","Epoch 15: val_loss did not improve from 1.03338\n","897/897 [==============================] - 29s 32ms/step - loss: 0.9907 - accuracy: 0.6315 - val_loss: 1.0963 - val_accuracy: 0.6088 - lr: 0.0010\n","Epoch 16/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9864 - accuracy: 0.6303\n","Epoch 16: val_loss did not improve from 1.03338\n","897/897 [==============================] - 29s 32ms/step - loss: 0.9864 - accuracy: 0.6303 - val_loss: 1.0342 - val_accuracy: 0.6223 - lr: 0.0010\n","Epoch 17/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9812 - accuracy: 0.6314\n","Epoch 17: val_loss improved from 1.03338 to 1.03062, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.17-0.62.hdf5\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9812 - accuracy: 0.6314 - val_loss: 1.0306 - val_accuracy: 0.6218 - lr: 0.0010\n","Epoch 18/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9685 - accuracy: 0.6385\n","Epoch 18: val_loss improved from 1.03062 to 1.02453, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.18-0.62.hdf5\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9684 - accuracy: 0.6384 - val_loss: 1.0245 - val_accuracy: 0.6241 - lr: 0.0010\n","Epoch 19/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9604 - accuracy: 0.6392\n","Epoch 19: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 32ms/step - loss: 0.9605 - accuracy: 0.6392 - val_loss: 1.0492 - val_accuracy: 0.6145 - lr: 0.0010\n","Epoch 20/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.6460\n","Epoch 20: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9518 - accuracy: 0.6460 - val_loss: 1.0286 - val_accuracy: 0.6286 - lr: 0.0010\n","Epoch 21/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9531 - accuracy: 0.6425\n","Epoch 21: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 32ms/step - loss: 0.9530 - accuracy: 0.6426 - val_loss: 1.0644 - val_accuracy: 0.6105 - lr: 0.0010\n","Epoch 22/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9435 - accuracy: 0.6464\n","Epoch 22: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9436 - accuracy: 0.6464 - val_loss: 1.0828 - val_accuracy: 0.6004 - lr: 0.0010\n","Epoch 23/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9361 - accuracy: 0.6507\n","Epoch 23: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9361 - accuracy: 0.6507 - val_loss: 1.0366 - val_accuracy: 0.6169 - lr: 0.0010\n","Epoch 24/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.9318 - accuracy: 0.6477\n","Epoch 24: val_loss did not improve from 1.02453\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9317 - accuracy: 0.6477 - val_loss: 1.0331 - val_accuracy: 0.6287 - lr: 0.0010\n","Epoch 25/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9288 - accuracy: 0.6519\n","Epoch 25: val_loss improved from 1.02453 to 1.01082, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.25-0.63.hdf5\n","897/897 [==============================] - 31s 34ms/step - loss: 0.9288 - accuracy: 0.6519 - val_loss: 1.0108 - val_accuracy: 0.6290 - lr: 0.0010\n","Epoch 26/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9178 - accuracy: 0.6564\n","Epoch 26: val_loss did not improve from 1.01082\n","897/897 [==============================] - 29s 32ms/step - loss: 0.9178 - accuracy: 0.6564 - val_loss: 1.0260 - val_accuracy: 0.6279 - lr: 0.0010\n","Epoch 27/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9111 - accuracy: 0.6607\n","Epoch 27: val_loss did not improve from 1.01082\n","897/897 [==============================] - 29s 33ms/step - loss: 0.9111 - accuracy: 0.6607 - val_loss: 1.0326 - val_accuracy: 0.6184 - lr: 0.0010\n","Epoch 28/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9084 - accuracy: 0.6615\n","Epoch 28: val_loss did not improve from 1.01082\n","897/897 [==============================] - 30s 34ms/step - loss: 0.9084 - accuracy: 0.6615 - val_loss: 1.0471 - val_accuracy: 0.6124 - lr: 0.0010\n","Epoch 29/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.9039 - accuracy: 0.6642\n","Epoch 29: val_loss improved from 1.01082 to 0.99271, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.29-0.63.hdf5\n","897/897 [==============================] - 31s 34ms/step - loss: 0.9039 - accuracy: 0.6642 - val_loss: 0.9927 - val_accuracy: 0.6335 - lr: 0.0010\n","Epoch 30/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.6641\n","Epoch 30: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 33ms/step - loss: 0.8962 - accuracy: 0.6641 - val_loss: 1.0244 - val_accuracy: 0.6307 - lr: 0.0010\n","Epoch 31/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8893 - accuracy: 0.6683\n","Epoch 31: val_loss did not improve from 0.99271\n","897/897 [==============================] - 29s 32ms/step - loss: 0.8893 - accuracy: 0.6683 - val_loss: 1.0049 - val_accuracy: 0.6303 - lr: 0.0010\n","Epoch 32/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8811 - accuracy: 0.6712\n","Epoch 32: val_loss did not improve from 0.99271\n","897/897 [==============================] - 31s 34ms/step - loss: 0.8811 - accuracy: 0.6712 - val_loss: 1.0020 - val_accuracy: 0.6362 - lr: 0.0010\n","Epoch 33/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8858 - accuracy: 0.6717\n","Epoch 33: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 33ms/step - loss: 0.8858 - accuracy: 0.6717 - val_loss: 1.0029 - val_accuracy: 0.6346 - lr: 0.0010\n","Epoch 34/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8765 - accuracy: 0.6731\n","Epoch 34: val_loss did not improve from 0.99271\n","897/897 [==============================] - 29s 32ms/step - loss: 0.8765 - accuracy: 0.6730 - val_loss: 1.0365 - val_accuracy: 0.6300 - lr: 0.0010\n","Epoch 35/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8704 - accuracy: 0.6774\n","Epoch 35: val_loss did not improve from 0.99271\n","897/897 [==============================] - 29s 32ms/step - loss: 0.8703 - accuracy: 0.6775 - val_loss: 1.0314 - val_accuracy: 0.6202 - lr: 0.0010\n","Epoch 36/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8678 - accuracy: 0.6773\n","Epoch 36: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 34ms/step - loss: 0.8675 - accuracy: 0.6774 - val_loss: 1.0032 - val_accuracy: 0.6332 - lr: 0.0010\n","Epoch 37/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8650 - accuracy: 0.6769\n","Epoch 37: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 34ms/step - loss: 0.8646 - accuracy: 0.6771 - val_loss: 1.0226 - val_accuracy: 0.6328 - lr: 0.0010\n","Epoch 38/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8557 - accuracy: 0.6823\n","Epoch 38: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 34ms/step - loss: 0.8557 - accuracy: 0.6823 - val_loss: 1.0458 - val_accuracy: 0.6296 - lr: 0.0010\n","Epoch 39/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8544 - accuracy: 0.6805\n","Epoch 39: val_loss did not improve from 0.99271\n","897/897 [==============================] - 31s 35ms/step - loss: 0.8545 - accuracy: 0.6806 - val_loss: 0.9989 - val_accuracy: 0.6484 - lr: 0.0010\n","Epoch 40/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.8516 - accuracy: 0.6849\n","Epoch 40: val_loss did not improve from 0.99271\n","897/897 [==============================] - 30s 33ms/step - loss: 0.8516 - accuracy: 0.6849 - val_loss: 1.0036 - val_accuracy: 0.6402 - lr: 0.0010\n","Epoch 41/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.8403 - accuracy: 0.6890\n","Epoch 41: val_loss did not improve from 0.99271\n","\n","Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","897/897 [==============================] - 29s 33ms/step - loss: 0.8405 - accuracy: 0.6890 - val_loss: 1.0579 - val_accuracy: 0.6264 - lr: 0.0010\n","Epoch 42/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.7764 - accuracy: 0.7114\n","Epoch 42: val_loss improved from 0.99271 to 0.94112, saving model to ../trained_models/increased_modules_experiments_5/fer2013_mini_XCEPTION.42-0.66.hdf5\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7764 - accuracy: 0.7114 - val_loss: 0.9411 - val_accuracy: 0.6645 - lr: 1.0000e-04\n","Epoch 43/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.7226\n","Epoch 43: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.7536 - accuracy: 0.7226 - val_loss: 0.9479 - val_accuracy: 0.6638 - lr: 1.0000e-04\n","Epoch 44/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7485 - accuracy: 0.7216\n","Epoch 44: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7483 - accuracy: 0.7218 - val_loss: 0.9512 - val_accuracy: 0.6654 - lr: 1.0000e-04\n","Epoch 45/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7419 - accuracy: 0.7249\n","Epoch 45: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.7422 - accuracy: 0.7248 - val_loss: 0.9461 - val_accuracy: 0.6637 - lr: 1.0000e-04\n","Epoch 46/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7295 - accuracy: 0.7316\n","Epoch 46: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7294 - accuracy: 0.7315 - val_loss: 0.9504 - val_accuracy: 0.6634 - lr: 1.0000e-04\n","Epoch 47/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7341 - accuracy: 0.7282\n","Epoch 47: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 34ms/step - loss: 0.7340 - accuracy: 0.7283 - val_loss: 0.9514 - val_accuracy: 0.6656 - lr: 1.0000e-04\n","Epoch 48/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7291 - accuracy: 0.7284\n","Epoch 48: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7288 - accuracy: 0.7284 - val_loss: 0.9518 - val_accuracy: 0.6624 - lr: 1.0000e-04\n","Epoch 49/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7251 - accuracy: 0.7312\n","Epoch 49: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 34ms/step - loss: 0.7252 - accuracy: 0.7312 - val_loss: 0.9537 - val_accuracy: 0.6670 - lr: 1.0000e-04\n","Epoch 50/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7201 - accuracy: 0.7346\n","Epoch 50: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.7201 - accuracy: 0.7346 - val_loss: 0.9526 - val_accuracy: 0.6620 - lr: 1.0000e-04\n","Epoch 51/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7211 - accuracy: 0.7331\n","Epoch 51: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7209 - accuracy: 0.7332 - val_loss: 0.9559 - val_accuracy: 0.6675 - lr: 1.0000e-04\n","Epoch 52/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7111 - accuracy: 0.7368\n","Epoch 52: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.7111 - accuracy: 0.7368 - val_loss: 0.9501 - val_accuracy: 0.6629 - lr: 1.0000e-04\n","Epoch 53/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.7372\n","Epoch 53: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.7078 - accuracy: 0.7373 - val_loss: 0.9525 - val_accuracy: 0.6651 - lr: 1.0000e-04\n","Epoch 54/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.7127 - accuracy: 0.7351\n","Epoch 54: val_loss did not improve from 0.94112\n","\n","Epoch 54: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","897/897 [==============================] - 31s 34ms/step - loss: 0.7127 - accuracy: 0.7351 - val_loss: 0.9504 - val_accuracy: 0.6676 - lr: 1.0000e-04\n","Epoch 55/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.7420\n","Epoch 55: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6975 - accuracy: 0.7420 - val_loss: 0.9515 - val_accuracy: 0.6638 - lr: 1.0000e-05\n","Epoch 56/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6959 - accuracy: 0.7427\n","Epoch 56: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6959 - accuracy: 0.7427 - val_loss: 0.9528 - val_accuracy: 0.6647 - lr: 1.0000e-05\n","Epoch 57/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6972 - accuracy: 0.7432\n","Epoch 57: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.6972 - accuracy: 0.7433 - val_loss: 0.9535 - val_accuracy: 0.6644 - lr: 1.0000e-05\n","Epoch 58/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6983 - accuracy: 0.7426\n","Epoch 58: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6983 - accuracy: 0.7427 - val_loss: 0.9539 - val_accuracy: 0.6648 - lr: 1.0000e-05\n","Epoch 59/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.7440\n","Epoch 59: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6945 - accuracy: 0.7440 - val_loss: 0.9525 - val_accuracy: 0.6647 - lr: 1.0000e-05\n","Epoch 60/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.7430\n","Epoch 60: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6952 - accuracy: 0.7430 - val_loss: 0.9521 - val_accuracy: 0.6648 - lr: 1.0000e-05\n","Epoch 61/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.7417\n","Epoch 61: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.6941 - accuracy: 0.7418 - val_loss: 0.9526 - val_accuracy: 0.6643 - lr: 1.0000e-05\n","Epoch 62/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6931 - accuracy: 0.7436\n","Epoch 62: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6931 - accuracy: 0.7436 - val_loss: 0.9544 - val_accuracy: 0.6654 - lr: 1.0000e-05\n","Epoch 63/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.7436\n","Epoch 63: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6918 - accuracy: 0.7436 - val_loss: 0.9544 - val_accuracy: 0.6652 - lr: 1.0000e-05\n","Epoch 64/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.7454\n","Epoch 64: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.6954 - accuracy: 0.7454 - val_loss: 0.9526 - val_accuracy: 0.6649 - lr: 1.0000e-05\n","Epoch 65/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6964 - accuracy: 0.7408\n","Epoch 65: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6963 - accuracy: 0.7409 - val_loss: 0.9537 - val_accuracy: 0.6661 - lr: 1.0000e-05\n","Epoch 66/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.7447\n","Epoch 66: val_loss did not improve from 0.94112\n","\n","Epoch 66: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6933 - accuracy: 0.7447 - val_loss: 0.9553 - val_accuracy: 0.6652 - lr: 1.0000e-05\n","Epoch 67/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.7461\n","Epoch 67: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6871 - accuracy: 0.7461 - val_loss: 0.9537 - val_accuracy: 0.6666 - lr: 1.0000e-06\n","Epoch 68/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.7438\n","Epoch 68: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 35ms/step - loss: 0.6891 - accuracy: 0.7438 - val_loss: 0.9558 - val_accuracy: 0.6668 - lr: 1.0000e-06\n","Epoch 69/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.7431\n","Epoch 69: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6877 - accuracy: 0.7430 - val_loss: 0.9540 - val_accuracy: 0.6665 - lr: 1.0000e-06\n","Epoch 70/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6943 - accuracy: 0.7425\n","Epoch 70: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6941 - accuracy: 0.7425 - val_loss: 0.9540 - val_accuracy: 0.6654 - lr: 1.0000e-06\n","Epoch 71/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7433\n","Epoch 71: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 35ms/step - loss: 0.6930 - accuracy: 0.7433 - val_loss: 0.9550 - val_accuracy: 0.6661 - lr: 1.0000e-06\n","Epoch 72/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6979 - accuracy: 0.7404\n","Epoch 72: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6980 - accuracy: 0.7404 - val_loss: 0.9569 - val_accuracy: 0.6669 - lr: 1.0000e-06\n","Epoch 73/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.7464\n","Epoch 73: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6902 - accuracy: 0.7464 - val_loss: 0.9555 - val_accuracy: 0.6662 - lr: 1.0000e-06\n","Epoch 74/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.7454\n","Epoch 74: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6903 - accuracy: 0.7454 - val_loss: 0.9559 - val_accuracy: 0.6663 - lr: 1.0000e-06\n","Epoch 75/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6865 - accuracy: 0.7454\n","Epoch 75: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.6864 - accuracy: 0.7456 - val_loss: 0.9538 - val_accuracy: 0.6666 - lr: 1.0000e-06\n","Epoch 76/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.7467\n","Epoch 76: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6894 - accuracy: 0.7467 - val_loss: 0.9542 - val_accuracy: 0.6668 - lr: 1.0000e-06\n","Epoch 77/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6910 - accuracy: 0.7436\n","Epoch 77: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6910 - accuracy: 0.7436 - val_loss: 0.9548 - val_accuracy: 0.6666 - lr: 1.0000e-06\n","Epoch 78/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.7449\n","Epoch 78: val_loss did not improve from 0.94112\n","\n","Epoch 78: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6927 - accuracy: 0.7448 - val_loss: 0.9546 - val_accuracy: 0.6663 - lr: 1.0000e-06\n","Epoch 79/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6862 - accuracy: 0.7453\n","Epoch 79: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6862 - accuracy: 0.7453 - val_loss: 0.9561 - val_accuracy: 0.6663 - lr: 1.0000e-07\n","Epoch 80/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6942 - accuracy: 0.7456\n","Epoch 80: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6943 - accuracy: 0.7456 - val_loss: 0.9557 - val_accuracy: 0.6668 - lr: 1.0000e-07\n","Epoch 81/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.7410\n","Epoch 81: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 32ms/step - loss: 0.6977 - accuracy: 0.7409 - val_loss: 0.9559 - val_accuracy: 0.6675 - lr: 1.0000e-07\n","Epoch 82/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6911 - accuracy: 0.7424\n","Epoch 82: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 34ms/step - loss: 0.6912 - accuracy: 0.7424 - val_loss: 0.9544 - val_accuracy: 0.6670 - lr: 1.0000e-07\n","Epoch 83/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6944 - accuracy: 0.7455\n","Epoch 83: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6941 - accuracy: 0.7456 - val_loss: 0.9546 - val_accuracy: 0.6668 - lr: 1.0000e-07\n","Epoch 84/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.7427\n","Epoch 84: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6938 - accuracy: 0.7427 - val_loss: 0.9566 - val_accuracy: 0.6670 - lr: 1.0000e-07\n","Epoch 85/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.7447\n","Epoch 85: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 34ms/step - loss: 0.6902 - accuracy: 0.7447 - val_loss: 0.9547 - val_accuracy: 0.6665 - lr: 1.0000e-07\n","Epoch 86/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.7445\n","Epoch 86: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6898 - accuracy: 0.7445 - val_loss: 0.9552 - val_accuracy: 0.6661 - lr: 1.0000e-07\n","Epoch 87/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6954 - accuracy: 0.7417\n","Epoch 87: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6956 - accuracy: 0.7416 - val_loss: 0.9551 - val_accuracy: 0.6672 - lr: 1.0000e-07\n","Epoch 88/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6908 - accuracy: 0.7463\n","Epoch 88: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 33ms/step - loss: 0.6909 - accuracy: 0.7463 - val_loss: 0.9553 - val_accuracy: 0.6661 - lr: 1.0000e-07\n","Epoch 89/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.7452\n","Epoch 89: val_loss did not improve from 0.94112\n","897/897 [==============================] - 30s 34ms/step - loss: 0.6918 - accuracy: 0.7452 - val_loss: 0.9550 - val_accuracy: 0.6669 - lr: 1.0000e-07\n","Epoch 90/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6894 - accuracy: 0.7443\n","Epoch 90: val_loss did not improve from 0.94112\n","\n","Epoch 90: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6894 - accuracy: 0.7443 - val_loss: 0.9546 - val_accuracy: 0.6668 - lr: 1.0000e-07\n","Epoch 91/10000\n","898/897 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.7457\n","Epoch 91: val_loss did not improve from 0.94112\n","897/897 [==============================] - 29s 33ms/step - loss: 0.6874 - accuracy: 0.7457 - val_loss: 0.9545 - val_accuracy: 0.6665 - lr: 1.0000e-08\n","Epoch 92/10000\n","897/897 [============================>.] - ETA: 0s - loss: 0.6910 - accuracy: 0.7414\n","Epoch 92: val_loss did not improve from 0.94112\n","897/897 [==============================] - 31s 34ms/step - loss: 0.6910 - accuracy: 0.7414 - val_loss: 0.9547 - val_accuracy: 0.6655 - lr: 1.0000e-08\n"]}],"source":["datasets = ['fer2013']\n","for dataset_name in datasets:\n","    print('Training dataset:', dataset_name)\n","\n","    # callbacks\n","    log_file_path = base_path + dataset_name + '_emotion_training.log'\n","    csv_logger = CSVLogger(log_file_path, append=False)\n","    early_stop = EarlyStopping('val_loss', patience=patience)\n","    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n","                                  patience=int(patience/4), verbose=1)\n","    trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n","    model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n","    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n","                                                    save_best_only=True)\n","    callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n","\n","    # loading dataset\n","    data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n","    faces, emotions = data_loader.get_data()\n","    faces = preprocess_input(faces)\n","    num_samples, num_classes = emotions.shape\n","    train_data, val_data = split_data(faces, emotions, validation_split)\n","    train_faces, train_emotions = train_data\n","    # model.fit_generator(data_generator.flow(train_faces, train_emotions,\n","    #                                         batch_size),\n","    #                     steps_per_epoch=len(train_faces) / batch_size,\n","    #                     epochs=num_epochs, verbose=1, callbacks=callbacks,\n","    #                     validation_data=val_data)\n","    history = model.fit(data_generator.flow(train_faces, train_emotions,\n","                                            batch_size),\n","                        steps_per_epoch=len(train_faces) / batch_size,\n","                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n","                        validation_data=val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uuEOO0H77O72"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"3scGopb500k0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"train_emotion_classifier.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}